# test_gguf.py -------------------------------------------------
# ! pip install --upgrade "llama-cpp-python==0.2.57"   # j√° vem com as libs nativas

from pathlib import Path
import pytest

try:
    from llama_cpp import Llama
except Exception:  # pragma: no cover - handled by test skip
    Llama = None

MODEL_FILE = Path("models/ggml-icd11-8b-V2-q4_k.gguf")

if Llama is not None:
    assert MODEL_FILE.exists(), f"Modelo {MODEL_FILE} n√£o encontrado"

    print("üì• Loading quantised GGUF ‚Ä¶")
    llm = Llama(
        model_path=str(MODEL_FILE),
        n_threads=8,       # todos os seus n√∫cleos
        n_batch=32,        # batches menores consomem menos RAM
        n_ctx=1536,        # contexto razo√°vel p/ 10 GB
        use_mmap=True,
        use_mlock=False,   # p√µe True se seu usu√°rio tiver permiss√£o
    )
    print("‚úÖ Model ready.\n")
else:
    llm = None

def query(concept: str, max_tokens: int = 1536, temp: float = 0.2):
    if llm is None:
        raise RuntimeError("llama_cpp is required to run this test")

    prompt = (
        "<s>[INST] Map the clinical concept to its ICD-11 code. "
        "Add extensions or cluster codes if needed: "
        f"{concept} [/INST]"
    )
    out = llm(prompt, max_tokens=max_tokens, temperature=temp)
    # `out["choices"][0]["text"]` j√° cont√©m somente a continua√ß√£o
    response = out["choices"][0]["text"].strip()
    return prompt, response


def test_query_produces_output():
    """Ensure the model returns some text for a sample concept."""
    pytest.importorskip("llama_cpp")
    _, response = query("Carcinoma in Situ of Skin of Perineum")
    assert response

if __name__ == "__main__":
    concept = "Carcinoma in Situ of Skin of Perineum"
    prompt, answer = query(concept)
    print("üì® Prompt:", prompt)
    print("ü§ñ Answer:", answer)